{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0809ad9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created!\n",
      "Loading our trained model from s3a://processed-data/models/f1_laptime_model...\n",
      "Model loaded successfully.\n",
      "Setting up stream to watch directory: s3a://raw-data/streaming-input/\n",
      "Stream is set up. Awaiting data...\n",
      "--- Streaming Service is LIVE ---\n",
      "Now, go to Notebook 10 and run it to trigger the stream!\n",
      "\n",
      "--- New Data Detected! (Batch ID: 0) ---\n",
      "Processing 3 new laps...\n",
      "Model Predictions:\n",
      "+------+--------+--------+-------+----------------+\n",
      "|Driver|Compound|TyreLife|LapTime|PredictedLapTime|\n",
      "+------+--------+--------+-------+----------------+\n",
      "|   VER|  MEDIUM|     2.0| 85.744|          92.129|\n",
      "|   VER|  MEDIUM|     3.0| 83.275|          91.977|\n",
      "|   VER|  MEDIUM|     4.0|   83.4|          91.825|\n",
      "+------+--------+--------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notebook: 09_Streaming_Prediction_Service.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "\n",
    "# --- 1. Configure and Start Spark Session ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 Streaming Prediction Service\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created!\")\n",
    "\n",
    "# --- 2. Load the Saved ML Model ---\n",
    "model_path = \"s3a://processed-data/models/f1_laptime_model\"\n",
    "print(f\"Loading our trained model from {model_path}...\")\n",
    "\n",
    "# Load the model you saved in the previous step\n",
    "model = PipelineModel.load(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- 3. Define the Stream Input Schema ---\n",
    "# Spark Streaming works best when you define the schema of the data it expects\n",
    "# This MUST match the Parquet files we've been creating\n",
    "# We only need the columns our model uses + a driver name for context\n",
    "schema = StructType([\n",
    "    StructField(\"Driver\", StringType(), True),\n",
    "    StructField(\"LapTime\", DoubleType(), True),\n",
    "    StructField(\"LapNumber\", DoubleType(), True),\n",
    "    StructField(\"TyreLife\", DoubleType(), True),\n",
    "    StructField(\"Compound\", StringType(), True),\n",
    "    StructField(\"IsAccurate\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# --- 4. Set Up the \"Real-Time\" Stream ---\n",
    "# We will \"watch\" this S3 directory for any new Parquet files\n",
    "input_dir = \"s3a://raw-data/streaming-input/\"\n",
    "print(f\"Setting up stream to watch directory: {input_dir}\")\n",
    "\n",
    "# Read from the directory as a stream\n",
    "# maxFilesPerTrigger=1 means \"process one new file at a time\"\n",
    "raw_stream_df = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(input_dir)\n",
    "\n",
    "print(\"Stream is set up. Awaiting data...\")\n",
    "\n",
    "# --- 5. Define the Processing Logic ---\n",
    "# This is what happens when a new file is detected\n",
    "def process_batch(batch_df, batch_id):\n",
    "    print(f\"\\n--- New Data Detected! (Batch ID: {batch_id}) ---\")\n",
    "    \n",
    "    # 1. Clean the new data\n",
    "    clean_df = batch_df.filter(\n",
    "        (F.col('IsAccurate') == True) &\n",
    "        (F.col('Compound') != 'UNKNOWN') &\n",
    "        (F.col('LapTime').isNotNull())\n",
    "    ).na.drop()\n",
    "    \n",
    "    if clean_df.count() == 0:\n",
    "        print(\"No valid data in this batch. Awaiting next file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {clean_df.count()} new laps...\")\n",
    "    \n",
    "    # 2. Apply our loaded ML model to make predictions\n",
    "    predictions = model.transform(clean_df)\n",
    "    \n",
    "    # 3. Show the results\n",
    "    print(\"Model Predictions:\")\n",
    "    predictions.select(\n",
    "        \"Driver\", \n",
    "        \"Compound\", \n",
    "        \"TyreLife\", \n",
    "        \"LapTime\",       # The actual, real lap time\n",
    "        F.round(\"prediction\", 3).alias(\"PredictedLapTime\") # Our model's guess\n",
    "    ).show()\n",
    "\n",
    "# --- 6. Start the Stream ---\n",
    "# This stream will run forever in your notebook, printing to the console\n",
    "# It uses process_batch function to handle data as it arrives\n",
    "query = raw_stream_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "print(\"--- Streaming Service is LIVE ---\")\n",
    "print(\"Now, go to Notebook 10 and run it to trigger the stream!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f8034-4ddb-4fd7-9230-830ab4cd0b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
