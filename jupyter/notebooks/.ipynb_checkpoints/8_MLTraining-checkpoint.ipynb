{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca2232",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook: 08_ML_Model_Training.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# --- 1. Configure and Start Spark Session ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 ML Model Training\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created!\")\n",
    "\n",
    "# --- 2. Load \"Massive\" Data from MinIO ---\n",
    "# This is the key: We read the *entire directory* as one DataFrame\n",
    "laps_dir = \"s3a://raw-data/laps/\"\n",
    "print(f\"Loading all lap data from {laps_dir}...\")\n",
    "\n",
    "df = spark.read.parquet(laps_dir)\n",
    "print(f\"Total laps loaded from all 24 races: {df.count()}\")\n",
    "\n",
    "# --- 3. Feature Engineering and Cleaning ---\n",
    "print(\"Cleaning data and preparing features...\")\n",
    "\n",
    "# Select only the columns we need\n",
    "# Label (what we predict): LapTime\n",
    "# Features (what we use to predict): LapNumber, TyreLife, Compound\n",
    "feature_df = df.select(\n",
    "    \"LapTime\", \n",
    "    \"LapNumber\", \n",
    "    \"TyreLife\", \n",
    "    \"Compound\",\n",
    "    \"IsAccurate\" # For filtering\n",
    ")\n",
    "\n",
    "# Clean the data\n",
    "# 1. Filter for \"good\" laps only\n",
    "clean_df = feature_df.filter(\n",
    "    (F.col('IsAccurate') == True) &\n",
    "    (F.col('LapTime').isNotNull()) &\n",
    "    (F.col('LapNumber').isNotNull()) &\n",
    "    (F.col('TyreLife').isNotNull()) &\n",
    "    (F.col('Compound').isNotNull()) &\n",
    "    (F.col('Compound') != 'UNKNOWN')\n",
    ")\n",
    "\n",
    "# 2. Drop any remaining nulls\n",
    "clean_df = clean_df.na.drop()\n",
    "\n",
    "print(f\"Total 'clean' laps for training: {clean_df.count()}\")\n",
    "clean_df.show(5)\n",
    "\n",
    "# --- 4. Define the ML Pipeline ---\n",
    "print(\"Defining ML pipeline...\")\n",
    "\n",
    "# Stage 1: Convert 'Compound' (SOFT, MEDIUM, HARD) to a number (e.g., 0.0, 1.0, 2.0)\n",
    "compound_indexer = StringIndexer(inputCol=\"Compound\", outputCol=\"CompoundIndex\")\n",
    "\n",
    "# Stage 2: Assemble all feature columns into a single \"features\" vector\n",
    "feature_cols = [\"LapNumber\", \"TyreLife\", \"CompoundIndex\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Stage 3: (Optional but good practice) Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Stage 4: Define the Machine Learning model\n",
    "# We're predicting LapTime (a number), so we use a regression model\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"LapTime\")\n",
    "\n",
    "# Chain all stages together into a single pipeline\n",
    "pipeline = Pipeline(stages=[compound_indexer, vector_assembler, scaler, lr])\n",
    "\n",
    "# --- 5. Train the Model ---\n",
    "print(\"Splitting data and training the model...\")\n",
    "(training_data, test_data) = clean_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# This is the \"product\": the trained model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 6. Evaluate the Model ---\n",
    "print(\"Evaluating model performance...\")\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show a few predictions\n",
    "predictions.select(\"LapTime\", \"prediction\", \"Compound\", \"TyreLife\").show(10)\n",
    "\n",
    "# Get the Root Mean Squared Error (RMSE)\n",
    "evaluator = RegressionEvaluator(labelCol=\"LapTime\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Model Performance (RMSE): {rmse}\")\n",
    "print(\"This means our model's predictions are, on average, off by {rmse} seconds.\")\n",
    "\n",
    "# --- 7. Save the \"Product\" (The Trained Model) ---\n",
    "model_path = \"s3a://processed-data/models/f1_laptime_model\"\n",
    "print(f\"Saving model to {model_path}...\")\n",
    "\n",
    "model.write().overwrite().save(model_path)\n",
    "\n",
    "print(\"--- ML Model Trained and Saved to MinIO! ---\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
